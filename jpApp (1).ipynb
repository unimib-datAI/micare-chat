{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Requirements**\n",
    "\n",
    "Tutorial from: [web](https://www.datacamp.com/tutorial/run-llama-3-locally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%pip install unstructured[docx] langchain langchainhub langchain_community langchain-chroma libmagic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variables**\n",
    "Change them to your path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#pdfToRead = \"C:/tmp/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading the documents**\n",
    "It is a best practice to develop and test your code in Jupyter Notebook before creating the app.\n",
    "\n",
    "We will load all the docx files from the folder using the DirectoryLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain_community.document_loaders import DirectoryLoader\n",
    "#from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "#file_path = \"./input/c06.pdf\"\n",
    "#fileExtToSearch = \"**/*.pdf\"\n",
    "#loader = PyPDFLoader(file_path)\n",
    "#books = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splitting the text**\n",
    "Feeding an entire book to the model is not feasible, as it would exceed its context window. To overcome this limitation, we must divide the text into smaller, more manageable chunks that fit comfortably within the model's context window.\n",
    "\n",
    "In our case, we will convert all four books to a chunk size of 500 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TextSplitter\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "class DisorderTextSplitter(TextSplitter):\n",
    "    def split_text(self, text: str) -> list[str]:\n",
    "        # Split the text into paragraphs\n",
    "        paragraphs = text.split('\\n')\n",
    "        \n",
    "        # Initialize variables\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        \n",
    "        for paragraph in paragraphs:\n",
    "            # Check if the paragraph starts with a disorder code (e.g., 6A00.3)\n",
    "            if re.match(r'^\\s*6[A-Z]\\d{2}(\\.\\d+)?([A-Z])?(?!\\s+ICD-11 MMS)', paragraph):\n",
    "                # If we have a current chunk, add it to the list of chunks\n",
    "                if current_chunk:\n",
    "                    chunks.append(current_chunk.strip())\n",
    "                # Start a new chunk with this paragraph\n",
    "                current_chunk = paragraph\n",
    "            else:\n",
    "                # If it's not a new disorder, add to the current chunk\n",
    "                current_chunk += \"\\n\" + paragraph\n",
    "        \n",
    "        # Add the last chunk if it exists\n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "        \n",
    "        return chunks\n",
    "# Usage\n",
    "with open('./input/c06.txt',encoding=\"utf-8\") as f:\n",
    "    content = f.read()\n",
    "    document = Document(page_content=content, metadata={})\n",
    "    text_splitter = DisorderTextSplitter()\n",
    "    all_splits = text_splitter.split_documents([document])    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "641"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(split_text(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ollama embeddings and Chroma vector store**\n",
    "We will use Langchain to convert the text into the embedding and store it in the Chroma database.\n",
    "\n",
    "We are using the Ollama Llama 3 model as an embedding model.\n",
    "\n",
    "\n",
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 641/641 [2:49:38<00:00, 15.88s/it]\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "#modelli = [\"gemma2:latest\",\"phi3:medium\",\"phi3.5:latest\",\"mistral-nemo:latest\",\"llama3:latest\"]\n",
    "modelli = [\"mixtral:8x7b\"]\n",
    "for modello in modelli:\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=all_splits,\n",
    "        embedding=OllamaEmbeddings(model=modello, show_progress=True),\n",
    "        persist_directory=f\"./chroma_db-{modello.replace(':','')}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TEST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is anxiety?\"\n",
    "docs = vectorstore.similarity_search(question)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building Langchain chains for Q&A retrieval system**\n",
    "To build a proper question-and-answer retrieval system, we will use Langchain chains and start adding the modules.\n",
    "\n",
    "In our Q&A chain, we will\n",
    "\n",
    "Use vector store as the retriever and format the results.\n",
    "After that, we will provide the RAG prompt. You can easily pull that from the Langchain Hub.\n",
    "Then, we will provide the Ollama Llama 3 inference function.\n",
    "In the end, we will parse the results only to display the response.\n",
    "Simply put, before passing it through the Llama 3 model, your question will be provided with context using the similarity search and RAG prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "llm = Ollama(model=\"llama3\")\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "qa_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing the Q&A retrieval chain**\n",
    "Ask relevant questions about books to understand more about the story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"from the context, which type of disease are being described?\"\n",
    "qa_chain.invoke(question)\n",
    "#question = \"which are anorexia symptoms?\"\n",
    "#qa_chain.invoke(question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
